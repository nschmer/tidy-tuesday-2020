---
title: 'Tidy tuesday 0: 2019-12-31'
author: "Natalie Schmer"
date: "12/31/2019"
output: html_document
editor_options: 
  chunk_output_type: console
---

I love Brooklyn 99, and my favorite episode is when Jake proposes to Amy (s05e04- HalloVeen) and I'm not great at text analysis sooo thought I'd give it a try! Cool cool cool cool cool no dount no doubt no doubt

These are all the packages I used, and how I read in my data. I copied and pasted the script into my text editor and saved that as a .rtf 

```{r}
{
  library(tidyverse)
  library(textreadr)
  library(SentimentAnalysis)
  library(tm)
  library(ggwordcloud)
  library(RColorBrewer)
}

#read in text file of sript
#script from: https://www.springfieldspringfield.co.uk/view_episode_scripts.php?tv-show=brooklyn-nine-nine&episode=s05e04

script <- striprtf::read_rtf("/Users/natalieschmer/Documents/GitHub/tidy-tuesday-2020/data/b99_halloveen_tt0.rtf")

```

This next part transforms the file and cleans it up, and I used a lot of the method from: https://towardsdatascience.com/a-light-introduction-to-text-analysis-in-r-ea291a9865a8

```{r}
#data cleaning
b99 <- tm::SimpleCorpus(VectorSource(script))

# 1. Stripping any extra white space:
b99 <- tm_map(b99, stripWhitespace)
# 2. Transforming everything to lowercase
b99 <- tm_map(b99, content_transformer(tolower))
# 3. Removing numbers 
b99 <- tm_map(b99, removeNumbers)
# 4. Removing punctuation
b99 <- tm_map(b99, removePunctuation)
# 5. Removing stop words
b99 <- tm_map(b99, removeWords, stopwords("english"))
```

Now, we analyze the word frequencies (again, borrowed from https://towardsdatascience.com/a-light-introduction-to-text-analysis-in-r-ea291a9865a8) 

I cut out a few words that I didn't think were that important, which is the slice(). Slice is from dplyr and it is so easy to select rows

```{r}

#Make into lists of words and calculate occurances 
DTM <- DocumentTermMatrix(b99)
sums <- as.data.frame(colSums(as.matrix(DTM)))
sums <- rownames_to_column(sums)
colnames(sums) <- c("term", "count")
sums <- dplyr::arrange(sums, desc(count))
sums <- sums %>% 
            slice(4:900)

#Get top 50 words to make plots 
head <- sums[1:50,]

```

Plotting, a bar chart and a word cloud  
used ggwordcloud: https://cran.r-project.org/web/packages/ggwordcloud/vignettes/ggwordcloud.html 

```{r}
#Bar chart 
ggplot(data= head, aes(x= reorder(term, -count), y= count))+
  geom_bar(stat = "identity")+
  ggthemes::theme_few()+
  theme(axis.text.x = element_text(angle = 70, hjust = 1))+
  labs(title ="50 Top words in HalloVeen (S5E4, Brooklyn 99)",
       y= "Count",
       x= "Word")

#Wordcloud 
ggplot(data=head, aes(label = term, size = count, color=count)) +
  geom_text_wordcloud_area(rm_outside = TRUE) +
  scale_size_area(max_size = 10) +
  theme_minimal()
```

I wanted to animate the word cloud but plotly::ggplotly(words) (aka my wordcloud) apparently is not integrated with ggwordcoulds yet :( (error: geom_GeomTextWordcloud() has yet to be implemented in plotly)

All I really did myself here was the plotting, as I borrowed the method for the actual text cleaning and analysis. But I still learned something new! 

```{r include=FALSE}
#non- ggplot word cloud
# 
# wordcloud::wordcloud(words = head$term, freq = head$count, min.freq = 1000,
#   max.words=100, random.order=FALSE, rot.per=0.01, 
#   colors=brewer.pal(8, "Dark2"))
```

